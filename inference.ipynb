{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ef67e3-a316-4448-b064-95bcfdddcc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8d26aad3f94703bd30d39228a4bc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6351711d074c3e8b89168a3867717a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07fd4cc545b420781666c6bc4d88fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "071c97e78d7d4fa283bd12a440dbb195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3966476\n",
      "601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print(\\'Training set\\')\\ncounts = filtered_sift_data.to_pandas()[\"data_source\"].value_counts()\\nprint(counts)\\nprint(\\'Evaluation set\\')\\ncounts = filtered_sift_data_eval.to_pandas()[\"data_source\"].value_counts()'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_utils import SIFT50MDataset\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "sift_dataset = load_dataset(\n",
    "    'amazon-agi/SIFT-50M',\n",
    "    #name='closed_ended_comparison',\n",
    "    name = 'closed_ended_content_level',\n",
    "    split='train',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "sift_dataset_test = load_dataset(\n",
    "    'amazon-agi/SIFT-50M',\n",
    "    #name='closed_ended_comparison',\n",
    "    name = 'closed_ended_content_level',\n",
    "    split='EvalSIFT',\n",
    "    trust_remote_code=True\n",
    ") \n",
    "\n",
    "# Define the allowed data sources\n",
    "allowed_values = [\"common_voice_de\", \"common_voice_en\", \"vctk_en\"]\n",
    "\n",
    "# Define a filter function\n",
    "def filter_data_source(example):\n",
    "    return example[\"data_source\"] in allowed_values\n",
    "\n",
    "filtered_sift_data = sift_dataset.filter(filter_data_source)\n",
    "filtered_sift_data_test = sift_dataset_test.filter(filter_data_source)\n",
    "\n",
    "#df = sift_dataset.select(range(1000)).to_pandas()\n",
    "print(len(filtered_sift_data))\n",
    "print(len(filtered_sift_data_test))\n",
    "#filtered_df = df[df[\"data_source\"].isin(allowed_values)]\n",
    "\n",
    "# Count number of entries per value\n",
    "#counts = filtered_df[\"data_source\"].value_counts()\n",
    "#print(counts)\n",
    "#sift_data = Dataset.from_pandas(filtered_df)\n",
    "# Define the base datasets paths (replace with your actual paths)\n",
    "\n",
    "'''print('Training set')\n",
    "counts = filtered_sift_data.to_pandas()[\"data_source\"].value_counts()\n",
    "print(counts)\n",
    "print('Evaluation set')\n",
    "counts = filtered_sift_data_eval.to_pandas()[\"data_source\"].value_counts()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf2cf9c-1be7-4a80-9dcb-331485be60e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_datasets_root = \"/home/jovyan/.cache/huggingface/datasets\"\n",
    "base_datasets_paths = {\n",
    "    \"common_voice_de\": None, # No longer needs a path, handled by load_dataset\n",
    "    #\"multilingual_librispeech_de\": None, # No longer needs a path, handled by load_dataset\n",
    "    \"common_voice_en\": None, # No longer needs a path, handled by load_dataset\n",
    "    \"vctk_en\": \"./vctk_corpus\" # VCTK still needs a root path for torchaudio\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa34cc89-15e8-4c0d-81ef-c12a2a61e862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'common_voice_de_25543288', 'messages': [{'role': 'user', 'content': [{'text': None, 'audio_path': '/home/jovyan/.cache/huggingface/datasets/downloads/extracted/e2d0a7ef8aa3a7692bd12936cfc8b5ca687d58a4fedbd18ac6820dbe1aef6b8d/de_test_0/common_voice_de_25543288.mp3', 'type': 'audio'}, {'text': 'Ist das Audio für ein erwachsenes Publikum geeignet?', 'audio_path': None, 'type': 'text'}]}, {'role': 'assistant', 'content': [{'text': 'Ja, das Audio ist für ein erwachsenes Publikum geeignet.', 'audio_path': None, 'type': 'text'}]}], 'task': 'closed_ended_content_level', 'data_source': 'common_voice_de'}\n"
     ]
    }
   ],
   "source": [
    "sift_iterable_dataset_eval = SIFT50MDataset(sift_dataset=filtered_sift_data_test, base_datasets_paths=base_datasets_paths, test_set=True)\n",
    "\n",
    "for x in sift_iterable_dataset_eval:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98144271-91b0-4b40-a7fe-ea011d9707c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Total samples for evaluation: 601\n",
      "--------------------------------------------------\n",
      "Printing Ground Truth for the first 5 samples:\n",
      "\n",
      "Sample 1 Ground Truth:\n",
      "Ist das Audio für ein erwachsenes Publikum geeignet? Ja, das Audio ist für ein erwachsenes Publikum geeignet.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2 Ground Truth:\n",
      "Um wie viele Uhr spielt die Frau in der Audiodatei über den Canal de la Marne au Rhin und die Bahnlinie Straßburg-Metz? Leider kann ich aus der Audio-Datei nicht hören, um wie viele Uhr die Frau spricht. Die Metadaten enthalten diese Information nicht.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3 Ground Truth:\n",
      "Was steht auf dem Pannenstreifen, das der Sprecher im Audioclip erwähnt? Auf dem Pannenstreifen steht ein defekter LKW.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 4 Ground Truth:\n",
      "Ist die Stimme auf dem Audioclip weiblich oder männlich? Die Stimme ist männlich.\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 5 Ground Truth:\n",
      "Gibt es Unterschiede zwischen den Geschlechtern in Bezug auf Suizidgedanken in Europa? Laut der vorliegenden Informationen gibt es keine Angaben zur Verteilung von Suizidgedanken zwischen den Geschlechtern in Europa.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sift_eval_list = list(sift_iterable_dataset_eval)\n",
    "print(f\"Dataset loaded. Total samples for evaluation: {len(sift_eval_list)}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"Printing Ground Truth for the first 5 samples:\")\n",
    "for i in range(min(5, len(sift_eval_list))):\n",
    "    sample = sift_eval_list[i]\n",
    "    messages = sample.get(\"messages\", [])\n",
    "    \n",
    "    if not messages:\n",
    "        continue\n",
    "    \n",
    "    # Build ground truth (question + answer)\n",
    "    ground_truth_texts = []\n",
    "    for message in messages:\n",
    "        for item in message.get(\"content\", []):\n",
    "            if item.get(\"type\") == \"text\" and item.get(\"text\"):\n",
    "                ground_truth_texts.append(item[\"text\"])\n",
    "    \n",
    "    gt_text = \" \".join(ground_truth_texts).strip()\n",
    "\n",
    "    print(f\"\\nSample {i+1} Ground Truth:\")\n",
    "    print(gt_text if gt_text else \"<EMPTY>\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd74ea42-ca1a-4e01-846a-715db221aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb107fc9902a4250853b42d03e25a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample 0 ===\n",
      "Ground truth transcript:\n",
      "Ist das Audio für ein erwachsenes Publikum geeignet? Ja, das Audio ist für ein erwachsenes Publikum geeignet.\n",
      "[clean] sample_0_clean.wav\n",
      "Generated text (cleaned): Ja, das Audio ist für ein erwachsenes Publikum geeignet. Anzahl der Wörter: 23\n",
      "WER: 0.706, CER: 0.523, BERTScore F1: 0.798\n",
      "--------------------------------------------------\n",
      "[noisy_0dB] sample_0_noisy_0dB.wav\n",
      "Generated text (cleaned): Ja, das Audio ist für ein erwachsenes Publikum geeignet. Ist es in deutscher Sprache?\n",
      "WER: 0.706, CER: 0.477, BERTScore F1: 0.842\n",
      "--------------------------------------------------\n",
      "[noisy_10dB] sample_0_noisy_10dB.wav\n",
      "Generated text (cleaned): Ja, das Audio ist für ein erwachsenes Publikum geeignet. Können Sie bitte beschreiben, was im Audio gesagt wird?\n",
      "WER: 0.706, CER: 0.495, BERTScore F1: 0.804\n",
      "--------------------------------------------------\n",
      "[noisy_20dB] sample_0_noisy_20dB.wav\n",
      "Generated text (cleaned): Ja, das Audio ist für ein erwachsenes Publikum geeignet. Was ist die Länge des Audio-Transkripts?\n",
      "WER: 0.647, CER: 0.450, BERTScore F1: 0.816\n",
      "--------------------------------------------------\n",
      "\n",
      "=== Sample 1 ===\n",
      "Ground truth transcript:\n",
      "Um wie viele Uhr spielt die Frau in der Audiodatei über den Canal de la Marne au Rhin und die Bahnlinie Straßburg-Metz? Leider kann ich aus der Audio-Datei nicht hören, um wie viele Uhr die Frau spricht. Die Metadaten enthalten diese Information nicht.\n",
      "[clean] sample_1_clean.wav\n",
      "Generated text (cleaned): Leider kann ich aus der Audio-Datei nicht hören, um wie viele Uhr die Frau spricht. Die Metadaten enthalten diese Information nicht. Neunzehn.\n",
      "WER: 0.535, CER: 0.516, BERTScore F1: 0.823\n",
      "--------------------------------------------------\n",
      "[noisy_0dB] sample_1_noisy_0dB.wav\n",
      "Generated text (cleaned): Leider kann ich aus der Audio-Datei nicht hören, um wie viele Uhr die Frau spricht. Die Metadaten enthalten diese Information nicht. Die genauen Informationen über die Länge des Abschnitts mit dem Kanal de la Marne au Rhin und der Bahnlinie Straßburg-Metz sind nicht verfügbar, da ich nur eine kurze Zusammenfassung ohne weitere Details habe.\n",
      "WER: 1.140, CER: 0.980, BERTScore F1: 0.834\n",
      "--------------------------------------------------\n",
      "[noisy_10dB] sample_1_noisy_10dB.wav\n",
      "Generated text (cleaned): Leider kann ich aus der Audio-Datei nicht hören, um wie viele Uhr die Frau spricht. Die Metadaten enthalten diese Information nicht. Nein, es ist nicht möglich, die Uhrzeit anhand der Informationen zu bestimmen, die ich habe. Ich kenne nur, dass eine Frau in der audiovisuellen Datei über den Canal de la Marne au Rhin und die Bahnlinie Straßburg-Metz spricht, aber keine weiteren Details.\n",
      "WER: 1.302, CER: 1.123, BERTScore F1: 0.850\n",
      "--------------------------------------------------\n",
      "[noisy_20dB] sample_1_noisy_20dB.wav\n",
      "Generated text (cleaned): Leider kann ich aus der Audio-Datei nicht hören, um wie viele Uhr die Frau spricht. Die Metadaten enthalten diese Information nicht. Neunzehn.\n",
      "WER: 0.535, CER: 0.516, BERTScore F1: 0.823\n",
      "--------------------------------------------------\n",
      "\n",
      "Final results table:\n",
      "  sample_idx         snr                                       ground_truth  \\\n",
      "0          0       clean  Ist das Audio für ein erwachsenes Publikum gee...   \n",
      "1          0   noisy_0dB  Ist das Audio für ein erwachsenes Publikum gee...   \n",
      "2          0  noisy_10dB  Ist das Audio für ein erwachsenes Publikum gee...   \n",
      "3          0  noisy_20dB  Ist das Audio für ein erwachsenes Publikum gee...   \n",
      "4          1       clean  Um wie viele Uhr spielt die Frau in der Audiod...   \n",
      "5          1   noisy_0dB  Um wie viele Uhr spielt die Frau in der Audiod...   \n",
      "6          1  noisy_10dB  Um wie viele Uhr spielt die Frau in der Audiod...   \n",
      "7          1  noisy_20dB  Um wie viele Uhr spielt die Frau in der Audiod...   \n",
      "\n",
      "                                      generated_text       wer       cer  \\\n",
      "0  Ja, das Audio ist für ein erwachsenes Publikum...  0.705882  0.522936   \n",
      "1  Ja, das Audio ist für ein erwachsenes Publikum...  0.705882  0.477064   \n",
      "2  Ja, das Audio ist für ein erwachsenes Publikum...  0.705882  0.495413   \n",
      "3  Ja, das Audio ist für ein erwachsenes Publikum...  0.647059  0.449541   \n",
      "4  Leider kann ich aus der Audio-Datei nicht höre...  0.534884  0.515873   \n",
      "5  Leider kann ich aus der Audio-Datei nicht höre...  1.139535  0.980159   \n",
      "6  Leider kann ich aus der Audio-Datei nicht höre...  1.302326  1.123016   \n",
      "7  Leider kann ich aus der Audio-Datei nicht höre...  0.534884  0.515873   \n",
      "\n",
      "   bertscore_f1  \n",
      "0      0.798256  \n",
      "1      0.842162  \n",
      "2      0.804225  \n",
      "3      0.815738  \n",
      "4      0.823068  \n",
      "5      0.834270  \n",
      "6      0.849781  \n",
      "7      0.823068  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration\n",
    "import evaluate\n",
    "\n",
    "# ----------------------\n",
    "# Paths\n",
    "# ----------------------\n",
    "data_dir = \"data/eval_audio_dataset\"\n",
    "snr_levels = [\"clean\", \"noisy_0dB\", \"noisy_10dB\", \"noisy_20dB\"]\n",
    "\n",
    "# ----------------------\n",
    "# Device (CPU only)\n",
    "# ----------------------\n",
    "device = \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ----------------------\n",
    "# Load processor & model\n",
    "# ----------------------\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n",
    "model = Qwen2AudioForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-Audio-7B-Instruct\",\n",
    "    trust_remote_code=True\n",
    ").eval()  # keep on CPU\n",
    "\n",
    "# ----------------------\n",
    "# Load metrics\n",
    "# ----------------------\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "# ----------------------\n",
    "# Helper: clean generated text\n",
    "# ----------------------\n",
    "def clean_generated_text(generated_text):\n",
    "    lines = generated_text.splitlines()\n",
    "    cleaned_lines = []\n",
    "    keep = False\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.lower().startswith(\"assistant\"):\n",
    "            keep = True\n",
    "            continue\n",
    "        if keep:\n",
    "            cleaned_lines.append(line)\n",
    "    return \" \".join(cleaned_lines).strip()\n",
    "\n",
    "# ----------------------\n",
    "# Group files by sample index\n",
    "# ----------------------\n",
    "files_by_idx = {}\n",
    "for fname in os.listdir(data_dir):\n",
    "    if not fname.endswith(\".wav\"):\n",
    "        continue\n",
    "    parts = fname.split(\"_\")\n",
    "    if len(parts) < 3:\n",
    "        continue\n",
    "    idx = parts[1]\n",
    "    files_by_idx.setdefault(idx, {})[fname] = os.path.join(data_dir, fname)\n",
    "\n",
    "# ----------------------\n",
    "# Collect results\n",
    "# ----------------------\n",
    "results = []\n",
    "for idx, files in sorted(files_by_idx.items(), key=lambda x: int(x[0]))[:2]:\n",
    "\n",
    "    # Retrieve dataset messages\n",
    "    sample = next((s for j, s in enumerate(sift_iterable_dataset_eval) if j == int(idx)), None)\n",
    "    if sample is None:\n",
    "        continue\n",
    "\n",
    "    messages = sample.get(\"messages\", [])\n",
    "    if not messages:\n",
    "        continue\n",
    "\n",
    "    # Build ground truth (question + answer)\n",
    "    ground_truth_texts = []\n",
    "    for message in messages:\n",
    "        for item in message.get(\"content\", []):\n",
    "            if item.get(\"type\") == \"text\" and item.get(\"text\"):\n",
    "                ground_truth_texts.append(item[\"text\"])\n",
    "    gt_text = \" \".join(ground_truth_texts).strip()\n",
    "\n",
    "    # Build conversation (with audio placeholders)\n",
    "    conversation = []\n",
    "    for message in messages:\n",
    "        role = message.get(\"role\", \"user\")\n",
    "        content = []\n",
    "        for item in message.get(\"content\", []):\n",
    "            if item.get(\"type\") == \"text\" and item.get(\"text\"):\n",
    "                content.append({\"type\": \"text\", \"text\": item[\"text\"]})\n",
    "            elif item.get(\"type\") == \"audio\":\n",
    "                content.append({\"type\": \"audio\"})\n",
    "        if content:\n",
    "            conversation.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    print(f\"\\n=== Sample {idx} ===\")\n",
    "    print(\"Ground truth transcript:\")\n",
    "    print(gt_text if gt_text else \"<EMPTY>\")\n",
    "\n",
    "    # ----------------------\n",
    "    # Loop over SNR levels\n",
    "    # ----------------------\n",
    "    for snr in snr_levels:\n",
    "        match = [f for f in files if snr in f]\n",
    "        if not match:\n",
    "            continue\n",
    "        audio_path = files[match[0]]\n",
    "\n",
    "        # Load audio\n",
    "        audio_input, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "        # Apply chat template\n",
    "        text_prompt = processor.apply_chat_template(\n",
    "            conversation, add_generation_prompt=True, tokenize=False\n",
    "        )\n",
    "\n",
    "        # Process audio + text\n",
    "        inputs = processor(\n",
    "            text=text_prompt,\n",
    "            audio=audio_input,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "        # Decode and clean\n",
    "        generated_text = processor.batch_decode(\n",
    "            generated_ids,\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        generated_text_clean = clean_generated_text(generated_text)\n",
    "\n",
    "        print(f\"[{snr}] {os.path.basename(audio_path)}\")\n",
    "        print(f\"Generated text (cleaned): {generated_text_clean}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # Evaluation\n",
    "        # ----------------------\n",
    "        if gt_text:\n",
    "            wer = wer_metric.compute(references=[gt_text], predictions=[generated_text_clean])\n",
    "            cer = cer_metric.compute(references=[gt_text], predictions=[generated_text_clean])\n",
    "            bertscore = bertscore_metric.compute(\n",
    "                predictions=[generated_text_clean], references=[gt_text], lang=\"de\"\n",
    "            )\n",
    "            results.append({\n",
    "                \"sample_idx\": idx,\n",
    "                \"snr\": snr,\n",
    "                \"ground_truth\": gt_text,\n",
    "                \"generated_text\": generated_text_clean,\n",
    "                \"wer\": wer,\n",
    "                \"cer\": cer,\n",
    "                \"bertscore_f1\": bertscore[\"f1\"][0]\n",
    "            })\n",
    "            print(f\"WER: {wer:.3f}, CER: {cer:.3f}, BERTScore F1: {bertscore['f1'][0]:.3f}\")\n",
    "        else:\n",
    "            results.append({\n",
    "                \"sample_idx\": idx,\n",
    "                \"snr\": snr,\n",
    "                \"ground_truth\": \"<EMPTY>\",\n",
    "                \"generated_text\": generated_text_clean,\n",
    "                \"wer\": None,\n",
    "                \"cer\": None,\n",
    "                \"bertscore_f1\": None\n",
    "            })\n",
    "            print(\"No ground truth for evaluation.\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# ----------------------\n",
    "# Convert to DataFrame\n",
    "# ----------------------\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\nFinal results table:\")\n",
    "print(df_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d0c28-0b7c-47a2-8358-0d1b57cf73f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-my_env]",
   "language": "python",
   "name": "conda-env-conda-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
