{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423502e4-1b4f-4e34-b984-b8fc55911d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c40b7aa1-ecf5-4a3a-b043-7c641317db39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "token_file_path = 'secrets.txt'\n",
    "\n",
    "if os.path.exists(token_file_path):\n",
    "    try:\n",
    "        with open(token_file_path, 'r') as f:\n",
    "            hf_token = f.read().strip()  # .strip() removes any leading/trailing whitespace\n",
    "\n",
    "        login(token=hf_token)\n",
    "        print(\"Successfully logged in to Hugging Face!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while trying to read the token file or log in: {e}\")\n",
    "else:\n",
    "    print(f\"Token file not found at {token_file_path}. Please create the file and add your token.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76bac49d-00a9-47a5-8935-8801b22e6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils import SIFT50MDataset\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "sift_dataset = load_dataset(\n",
    "    'amazon-agi/SIFT-50M',\n",
    "    #name='closed_ended_comparison',\n",
    "    name = 'closed_ended_content_level',\n",
    "    split='train',\n",
    "    trust_remote_code=True\n",
    ")\n",
    " \n",
    "\n",
    "# Define the allowed data sources\n",
    "allowed_values = [\"common_voice_de\", \"common_voice_en\", \"vctk_en\"]\n",
    "\n",
    "# Define a filter function\n",
    "def filter_data_source(example):\n",
    "    return example[\"data_source\"] in allowed_values\n",
    "\n",
    "filtered_sift_data = sift_dataset.filter(filter_data_source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7531d56-80ac-4274-a3f2-dbf4734a4fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3966476"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_sift_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd097f2-8edd-42bc-8264-f425a6f7294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filtered_sift_data = filtered_sift_data.add_column(\n",
    "    \"data_source_str\", filtered_sift_data[\"data_source\"]\n",
    ")\n",
    "\n",
    "stratifiable_dataset = filtered_sift_data.class_encode_column(\"data_source\")\n",
    "\n",
    "train_validation_split = stratifiable_dataset.train_test_split(\n",
    "    test_size=0.2,\n",
    "    seed=40,\n",
    "    stratify_by_column=\"data_source\"\n",
    ")\n",
    "\n",
    "train_ds, eval_ds = train_validation_split[\"train\"], train_validation_split[\"test\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b9844c-482b-4142-95c4-d9071498b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_ds, eval_ds = train_validation_split[\"train\"], train_validation_split[\"test\"]\n",
    "\n",
    "train_ds = train_ds.remove_columns(\"data_source\").rename_column(\"data_source_str\", \"data_source\")\n",
    "eval_ds  = eval_ds.remove_columns(\"data_source\").rename_column(\"data_source_str\", \"data_source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "112c9886-bb87-4bb5-8ca5-c7c766a54130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "793296"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452c52dc-c337-4202-bdeb-55c340ff863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_datasets_root = \"/home/jovyan/.cache/huggingface/datasets\"\n",
    "base_datasets_paths = {\n",
    "    \"common_voice_de\": None, # No longer needs a path, handled by load_dataset\n",
    "    #\"multilingual_librispeech_de\": None, # No longer needs a path, handled by load_dataset\n",
    "    \"common_voice_en\": None, # No longer needs a path, handled by load_dataset\n",
    "    \"vctk_en\": \"./vctk_corpus\" # VCTK still needs a root path for torchaudio\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "576e0873-2407-4992-a227-29c4b3324701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_filtered_hf_dataset(sift_iterable_dataset_raw):\n",
    "    \"\"\"\n",
    "    1. Iterates through the streaming SIFT50MDataset, which already handles \n",
    "       path resolution and internal validity checks.\n",
    "    2. Collects all the valid entries into a Python list.\n",
    "    3. Converts the collected list into a Hugging Face Dataset object.\n",
    "    \"\"\"\n",
    "    print(\"Starting collection and conversion of evaluation data...\")\n",
    "    \n",
    "    valid_entries = []\n",
    "    \n",
    "    try:\n",
    "        for entry in tqdm(sift_iterable_dataset_raw, desc=\"Collecting valid eval samples\"):\n",
    "            valid_entries.append(entry)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during data collection: {e}\")\n",
    "        # The process will still convert whatever valid data was collected before the error.\n",
    "\n",
    "    print(f\"Finished collecting {len(valid_entries)} valid entries.\")\n",
    "    \n",
    "    if not valid_entries:\n",
    "        print(\"Warning: Evaluation dataset is empty after filtering. Cannot create Dataset object.\")\n",
    "        return None\n",
    "        \n",
    "    filtered_dataset = Dataset.from_list(valid_entries)\n",
    "    \n",
    "    # Perform cleanup after loading a large object\n",
    "    del valid_entries\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "414e5396-564c-4ab0-a043-d84c3babe94c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting collection and conversion of evaluation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting valid eval samples: 28249it [14:40, 32.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting 28249 valid entries.\n"
     ]
    }
   ],
   "source": [
    "from src.data_utils import SIFT50MDataset\n",
    "\n",
    "sift_iterable_dataset_raw_eval = SIFT50MDataset(\n",
    "    sift_dataset=eval_ds.select(range(30000)), \n",
    "    base_datasets_paths=base_datasets_paths\n",
    ")\n",
    "\n",
    "sift_iterable_dataset_eval_filtered = create_filtered_hf_dataset(sift_iterable_dataset_raw_eval)\n",
    "\n",
    "#print(sift_iterable_dataset_eval_filtered[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd56f307-778d-4da6-a0ff-38950703c887",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    " \n",
    "from src.train_qlora import train_model\n",
    "from transformers import AutoProcessor\n",
    "from src.data_utils import SIFT50MDataset\n",
    "from src.data_collator import CustomDataCollator\n",
    "import torch # Import torch for CUDA memory management\n",
    "import gc    # Import garbage collector\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "\n",
    "    processor = None\n",
    "    data_collator = None\n",
    "    sift_iterable_dataset_eval = None\n",
    "    sift_iterable_dataset_train = None\n",
    "    \n",
    "    try:\n",
    "        # 2. Setup (inside try block)\n",
    "        print(\"Starting model setup...\")\n",
    "        processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\", sampling_rate=16000)\n",
    "        data_collator = CustomDataCollator(processor)\n",
    "\n",
    "        sift_iterable_dataset_eval = SIFT50MDataset(sift_dataset=sift_iterable_dataset_eval_filtered, base_datasets_paths=base_datasets_paths)\n",
    "        sift_iterable_dataset_train = SIFT50MDataset(sift_dataset=train_ds.select(range(1000000)), base_datasets_paths=base_datasets_paths)\n",
    "\n",
    "        print(\"Starting model training...\")\n",
    "        train_model(\n",
    "            eval_ds=sift_iterable_dataset_eval,\n",
    "            train_ds=sift_iterable_dataset_train,\n",
    "            processor=processor,\n",
    "            custom_data_collator=data_collator,\n",
    "            resume=True\n",
    "        )\n",
    "        print(\"Training completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error or exception occurred during execution: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    finally:\n",
    "        print(\"Starting cleanup and memory release...\")\n",
    "        \n",
    "        # Clear object references to aid garbage collection\n",
    "        del processor\n",
    "        del data_collator\n",
    "        del sift_iterable_dataset_eval\n",
    "        del sift_iterable_dataset_train\n",
    "        \n",
    "        # Force Python's garbage collector to run\n",
    "        gc.collect() \n",
    "        \n",
    "        # Release unused GPU memory cached by PyTorch\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"PyTorch CUDA memory cache released.\")\n",
    "        \n",
    "        print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83b593d1-3d72-4144-a161-904708cebfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model setup...\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:14<00:00,  2.86s/it]\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'pad_token_id': 151643}.\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\tper_device_train_batch_size: 4 (from args) != 2 (from trainer_state.json)\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 4:54:24, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.541900</td>\n",
       "      <td>0.619307</td>\n",
       "      <td>3.570768</td>\n",
       "      <td>154916.000000</td>\n",
       "      <td>0.843831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>0.481996</td>\n",
       "      <td>3.549057</td>\n",
       "      <td>312928.000000</td>\n",
       "      <td>0.873887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.416185</td>\n",
       "      <td>3.705520</td>\n",
       "      <td>471779.000000</td>\n",
       "      <td>0.885401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/jovyan/.local/lib/python3.11/site-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed successfully.\n",
      "Starting cleanup and memory release...\n",
      "PyTorch CUDA memory cache released.\n",
      "Cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ff5006f-f60f-499c-85ae-5cf2e801a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "del main\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "846a1adb-291d-47f1-b173-9661ada388d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a6b0a-f17f-4ac3-b14d-f079ef11a3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:conda-ainmc]",
   "language": "python",
   "name": "conda-env-conda-ainmc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
